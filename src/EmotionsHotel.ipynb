{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3901bcc",
   "metadata": {},
   "source": [
    "–¶–µ–ª—å - –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ —Å –∏—Å–ø–æ—å–∑–æ–≤–∞–Ω–∏–µ–º \n",
    "–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ NPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdff429",
   "metadata": {},
   "source": [
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782ca69",
   "metadata": {},
   "source": [
    "the organization of the data is carried out - –í—ã–ø–æ–ª—å–Ω–∞–µ—Ç—Å—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2653dc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['c:\\\\Users\\\\Usuario\\\\Videos\\\\Innovation Web\\\\Project\\\\analis_emotion\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', 'pandas', 'numpy', 'scikit-learn'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instalation of libraries - —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import sys\n",
    "import subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"numpy\", \"scikit-learn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56784af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas 2.3.3 y NumPy 2.3.5 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(f\"Pandas {pd.__version__} y NumPy {np.__version__} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf2c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Date ===\n",
      "              0              1             2             3       4        5  \\\n",
      "0  –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞  –¶–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ  –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ  –ö–∞—á–µ—Å—Ç–≤–æ —Å–Ω–∞  –ù–æ–º–µ—Ä–∞  –ß–∏—Å—Ç–æ—Ç–∞   \n",
      "1             5              5             5           NaN       5        5   \n",
      "2             5              5             5             5       5        5   \n",
      "\n",
      "              6                                                  7  \n",
      "0  –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ                                       –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞  \n",
      "1             5  –≠–ª–µ–≥–∞–Ω—Ç–Ω—ã–π –æ—Ç–µ–ª—å –≤ —Å–∞–º–æ–º —Ü–µ–Ω—Ç—Ä–µ –î—É–±–∞–π. –ö—Ä–∞—Å–∏–≤—ã...  \n",
      "2             5  –ï—Å–ª–∏ —Å—Ç–∏–ª—å –≤–∞—à–µ–π –ø–æ–µ–∑–¥–∫–∏ –≤ –î—É–±–∞–π-- —ç—Ç–æ —à–æ–ø–ø–∏–Ω–≥...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw/Test_data_marked.csv', header=None, encoding='utf-8')\n",
    "\n",
    "print(\"\\n=== Date ===\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39001208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mber of text: 6877\n"
     ]
    }
   ],
   "source": [
    "column_text = df[7]\n",
    "print(f\"N√∫mber of text: {len(column_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f1169df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    –≠–ª–µ–≥–∞–Ω—Ç–Ω—ã–π –æ—Ç–µ–ª—å –≤ —Å–∞–º–æ–º —Ü–µ–Ω—Ç—Ä–µ –î—É–±–∞–π. –ö—Ä–∞—Å–∏–≤—ã...\n",
      "1    –ï—Å–ª–∏ —Å—Ç–∏–ª—å –≤–∞—à–µ–π –ø–æ–µ–∑–¥–∫–∏ –≤ –î—É–±–∞–π-- —ç—Ç–æ —à–æ–ø–ø–∏–Ω–≥...\n",
      "2    –ü—Ä–æ–≤–µ–ª–∏ —Å —Å—É–ø—Ä—É–≥–æ–π —Ç–∞–º —É–∏–∫ –µ–Ω–¥. –ë—Ä–∞–ª–∏ —Å–∞–º—ã–π –ª—é...\n",
      "3    –ü–æ –¥–æ—Ä–æ–≥–µ –¥–æ–º–æ–π –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∫–∞–Ω—Ç–æ–≤–∞—Ç—å—Å...\n",
      "4    –º–µ–Ω—è –≤–ø–µ—á–∞—Ç–ª–∏–ª–∏ —Ä–∞–∑–º–µ—Ä—ã! —Å–∞–º—ã–π –æ–≥—Ä–æ–º–Ω—ã–π –Ω–æ–º–µ—Ä,...\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#—É–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É\n",
    "text_cleaning = column_text .iloc[1:].reset_index(drop=True) \n",
    "print(text_cleaning.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f30147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== after filtering - –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ===\n",
      "–¢–µ–∫—Å—Ç—ã, –ø—Ä–∏–≥–æ–¥–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: 6876\n",
      "\n",
      "=== DATAFRAME CON TEXTOS LIMPIOS ===\n",
      "                                          text_russi\n",
      "0  –≠–ª–µ–≥–∞–Ω—Ç–Ω—ã–π –æ—Ç–µ–ª—å –≤ —Å–∞–º–æ–º —Ü–µ–Ω—Ç—Ä–µ –î—É–±–∞–π. –ö—Ä–∞—Å–∏–≤—ã...\n",
      "1  –ï—Å–ª–∏ —Å—Ç–∏–ª—å –≤–∞—à–µ–π –ø–æ–µ–∑–¥–∫–∏ –≤ –î—É–±–∞–π-- —ç—Ç–æ —à–æ–ø–ø–∏–Ω–≥...\n",
      "2  –ü—Ä–æ–≤–µ–ª–∏ —Å —Å—É–ø—Ä—É–≥–æ–π —Ç–∞–º —É–∏–∫ –µ–Ω–¥. –ë—Ä–∞–ª–∏ —Å–∞–º—ã–π –ª—é...\n",
      "3  –ü–æ –¥–æ—Ä–æ–≥–µ –¥–æ–º–æ–π –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∫–∞–Ω—Ç–æ–≤–∞—Ç—å—Å...\n",
      "4  –º–µ–Ω—è –≤–ø–µ—á–∞—Ç–ª–∏–ª–∏ —Ä–∞–∑–º–µ—Ä—ã! —Å–∞–º—ã–π –æ–≥—Ä–æ–º–Ω—ã–π –Ω–æ–º–µ—Ä,...\n",
      "\n",
      "Dimensiones: (6876, 1)\n"
     ]
    }
   ],
   "source": [
    "text_cleaning = text_cleaning.apply(lambda x: str(x).strip() if pd.notnull(x) else \"\")\n",
    "\n",
    "# 3. Filtrar solo textos no vac√≠os\n",
    "text_cleaning = text_cleaning[text_cleaning.str.len() > 0]\n",
    "\n",
    "print(f\"\\n=== after filtering - –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ===\")\n",
    "print(f\"–¢–µ–∫—Å—Ç—ã, –ø—Ä–∏–≥–æ–¥–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(text_cleaning)}\")\n",
    "\n",
    "# 4. Crear un DataFrame solo con los textos (para seguir trabajando)\n",
    "df_textos = pd.DataFrame({'text_russi': text_cleaning})\n",
    "\n",
    "print(\"\\n=== DATAFRAME CON TEXTOS LIMPIOS ===\")\n",
    "print(df_textos.head())\n",
    "print(f\"\\nDimensiones: {df_textos.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115a9cc",
   "metadata": {},
   "source": [
    "# –¢okenization - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001c243",
   "metadata": {},
   "source": [
    "–ú—ã –∏—Å–ø—É–ª–∑—É–µ–º –º–µ—Ç–æ–¥ N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb59cc",
   "metadata": {},
   "source": [
    "–î–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º—ã –∑–∞–≥—Ä—É–∂–∞–µ–º —è–∑—ã–∫–æ–≤—ã–µ —Ñ–∞–π–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∏ —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å. –≠—Ç–∏ —Ñ–∞–π–ª—ã —Å–ª—É–∂–∞—Ç –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –∞–ª–≥–æ—Ä–∏—Ç–º –º–æ–≥ —Ç–æ—á–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç.\"\n",
    "\n",
    "\"To perform tokenization correctly, we download language files containing grammatical rules and vocabulary. These files serve as a knowledge base so the algorithm can accurately understand and process Russian text.\" \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6c4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Descargar los datos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48122524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Russian stopwords / –ü–æ–ª—É—á–∏—Ç—å —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "\n",
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd474ca",
   "metadata": {},
   "source": [
    "Function to clean and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f540d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text):\n",
    "     # Tokenize / –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å\n",
    "    tokens = word_tokenize(str(text), language='russian')\n",
    "    # Lowercase and filter / –ù–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "\n",
    "    clean_tokens = [word.lower() for word in tokens \n",
    "                   if word.lower() not in russian_stopwords \n",
    "                   and word.isalpha()]\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "878b19fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado: —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–π –æ—Ç–µ–ª—å —Å–∞–º–æ–º —Ü–µ–Ω—Ç—Ä–µ –¥—É–±–∞–π –∫—Ä–∞—Å–∏–≤—ã–µ –Ω–æ–º–µ—Ä–∞\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(pattern, '', str(text))\n",
    "    \n",
    "    # Min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # CORRECCI√ìN: Mantener letras rusas y espacios\n",
    "    text = re.sub(r'[^–∞-—è—ë\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    \n",
    "    # Eliminar stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # CORRECCI√ìN: Unir tokens correctamente\n",
    "    clean_text = ' '.join(tokens)  # ¬°Esto faltaba!\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Probar\n",
    "test_text = \"–≠–ª–µ–≥–∞–Ω—Ç–Ω—ã–π –æ—Ç–µ–ª—å –≤ —Å–∞–º–æ–º —Ü–µ–Ω—Ç—Ä–µ –î—É–±–∞–π. –ö—Ä–∞—Å–∏–≤—ã–µ –Ω–æ–º–µ—Ä–∞.\"\n",
    "result = preprocess_text(test_text)\n",
    "print(\"Resultado:\", result)\n",
    "\n",
    "df_textos['tokens'] = df_textos['text_russi'].apply(clean_tokenize)\n",
    "df_textos['clear_text'] = df_textos['text_russi'].apply(preprocess_text)\n",
    "\n",
    "total_tokens = sum(len(tokens) for tokens in df_textos['tokens'])\n",
    "promedio_tokens = total_tokens / len(df_textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efcd89b",
   "metadata": {},
   "source": [
    "–ø–µ—Ä–µ–º–æ—Å—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - review of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a350854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada: 6876 rese√±as\n"
     ]
    }
   ],
   "source": [
    "df_textos['tokens'] = df_textos['text_russi'].apply(clean_tokenize)\n",
    "print(f\"‚úÖ Tokenizaci√≥n completada: {len(df_textos)} rese√±as\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef3e76",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d294ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stemming aplicado a 6876 rese√±as\n",
      "\n",
      "Ejemplo de resultados:\n",
      "Original: ['—ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–π', '–æ—Ç–µ–ª—å', '—Å–∞–º–æ–º', '—Ü–µ–Ω—Ç—Ä–µ', '–¥—É–±–∞–π', '–∫—Ä–∞—Å–∏–≤—ã–µ', '—Å—Ç–∏–ª—å–Ω—ã–µ', '–Ω–æ–º–µ—Ä–∞']\n",
      "Stemmed: ['—ç–ª–µ–≥–∞–Ω—Ç–Ω', '–æ—Ç–µ–ª', '—Å–∞–º', '—Ü–µ–Ω—Ç—Ä', '–¥—É–±–∞', '–∫—Ä–∞—Å–∏–≤', '—Å—Ç–∏–ª—å–Ω', '–Ω–æ–º–µ—Ä']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#–°–æ–∑–¥–∞–µ–º —Å—Ç–µ–º–º–∏–≥ –¥–ª—è –¥–∞–Ω–Ω—ã–µ \n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def stem_tokens(tokens_list):\n",
    "    \"\"\"Stemming russian\"\"\"\n",
    "    return [stemmer.stem(token) for token in tokens_list]\n",
    "\n",
    "#aplicamos toda la columna tokens del dataFrame\n",
    "df_textos['tokens_stemmed'] = df_textos['tokens'].apply(stem_tokens)\n",
    "\n",
    "print(\"‚úÖ Stemming aplicado a\", len(df_textos), \"rese√±as\")\n",
    "print(\"\\nEjemplo de resultados:\")\n",
    "print(\"Original:\", df_textos['tokens'].iloc[0][:8])\n",
    "print(\"Stemmed:\", df_textos['tokens_stemmed'].iloc[0][:8])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ba795c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ESTADO ACTUAL: PREPROCESAMIENTO COMPLETADO\n",
      "==================================================\n",
      "DataFrame tiene 6876 filas\n",
      "Columnas disponibles: ['text_russi', 'tokens', 'clear_text', 'tokens_stemmed', 'texto_stemmed']\n"
     ]
    }
   ],
   "source": [
    "df_textos['texto_stemmed'] = df_textos['tokens_stemmed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTADO ACTUAL: PREPROCESAMIENTO COMPLETADO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"DataFrame tiene {len(df_textos)} filas\")\n",
    "print(\"Columnas disponibles:\", df_textos.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a41229",
   "metadata": {},
   "source": [
    "Cargamos datos positivos y negative, que serviran para entrenar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39a3bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "# positive['sentiment'] = 'positive'\n",
    "\n",
    "# negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "# negative['sentiment'] = 'negative'\n",
    "\n",
    "# df_labeled = pd.concat([positive, negative], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a01c181",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3660d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 1. Preparar datos: unir tokens stemmeados en texto\n",
    "df_textos['texto_stemmed'] = df_textos['tokens_stemmed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 2. Crear vectorizador con unigramas y bigramas\n",
    "vectorizador = CountVectorizer(ngram_range=(1, 2))\n",
    "X_ngrams = vectorizador.fit_transform(df_textos['texto_stemmed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa5640a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MATRIZ N-GRAMS: 6876 documentos √ó 236417 n-grams\n",
      "üìä Total n-grams √∫nicos: 236417\n",
      "üî§ Unigramas: 18487\n",
      "üî§ Bigramas: 217930\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ MATRIZ N-GRAMS: {X_ngrams.shape[0]} documentos √ó {X_ngrams.shape[1]} n-grams\")\n",
    "print(f\"üìä Total n-grams √∫nicos: {X_ngrams.shape[1]}\")\n",
    "print(f\"üî§ Unigramas: {sum(1 for x in vectorizador.get_feature_names_out() if len(x.split()) == 1)}\")\n",
    "print(f\"üî§ Bigramas: {sum(1 for x in vectorizador.get_feature_names_out() if len(x.split()) == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188d9b3",
   "metadata": {},
   "source": [
    "# Vectorization - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f536053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Crear vectorizador TF-IDF con unigramas y bigramas\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Unigramas y bigramas\n",
    "    min_df=2,                # Ignorar t√©rminos en menos de 2 documentos\n",
    "    max_df=0.95,             # Ignorar t√©rminos en m√°s del 95% de documentos\n",
    "    use_idf=True             # Usar Inverse Document Frequency\n",
    ")\n",
    "\n",
    "# 2. Aplicar TF-IDF a los textos stemmeados\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_textos['texto_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cba350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6877\n",
      "label\n",
      "positive    3455\n",
      "negative    3422\n",
      "Name: count, dtype: int64\n",
      "                                                   text     label\n",
      "0     —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–π –æ—Ç–µ–ª—å —Å–∞–º–æ–º —Ü–µ–Ω—Ç—Ä–µ –¥—É–±–∞–π –∫—Ä–∞—Å–∏–≤—ã–µ —Å...  positive\n",
      "1     —Å—Ç–∏–ª—å –≤–∞—à–µ–π –ø–æ–µ–∑–¥–∫–∏ –¥—É–±–∞–π —ç—Ç–æ —à–æ–ø–ø–∏–Ω–≥ –¥—É–±–∞–π –º–æ...  negative\n",
      "2     –ø—Ä–æ–≤–µ–ª–∏ —Å—É–ø—Ä—É–≥–æ–π —É–∏–∫ –µ–Ω–¥ –±—Ä–∞–ª–∏ —Å–∞–º—ã–π –ª—é–∫—Å –≤–∏–¥–æ...  negative\n",
      "3     –¥–æ—Ä–æ–≥–µ –¥–æ–º–æ–π –ø–µ—Ä–µ–∫–∞–Ω—Ç–æ–≤–∞—Ç—å—Å—è –≤—Ä–µ–º—è –ø–µ—Ä–µ—Å–∞–¥–∫–∏ —Ä...  negative\n",
      "4     –≤–ø–µ—á–∞—Ç–ª–∏–ª–∏ —Ä–∞–∑–º–µ—Ä—ã —Å–∞–º—ã–π –æ–≥—Ä–æ–º–Ω—ã–π –Ω–æ–º–µ—Ä –∫–æ—Ç–æ—Ä–æ...  positive\n",
      "...                                                 ...       ...\n",
      "6872  –æ—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è –æ—Ç–µ–ª—å —Ö–æ—Ä–æ—à–∏–µ –Ω–æ–º–µ—Ä–∞ –∫—É—Ö–Ω–µ–π –≤...  negative\n",
      "6873  –ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ –±—ã–≤–∞—é –æ—á–µ–Ω—å —á–∞—Å—Ç–æ –ø–æ—ç—Ç–æ–º—É –ø–æ—è–≤–ª–µ–Ω–∏–µ...  positive\n",
      "6874  –∏–∑—É–º–∏—Ç–µ–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ –æ–∂–∏–¥–∞–ª–∞ —Ç–∞–∫–æ–≥–æ –ø–æ–Ω—Ä–∞...  negative\n",
      "6875  –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª–∏—Å—å –æ—Ç–µ–ª–µ –∞–≤–≥—É—Å—Ç–µ —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ ...  positive\n",
      "6876                                                NaN  positive\n",
      "\n",
      "[6877 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your cleaned hotel data\n",
    "#df_hotel = pd.read_csv('../data/raw/Test_data_marked.csv')  # Adjust filename\n",
    "\n",
    "# Create dummy labels (positive/negative) -\n",
    "np.random.seed(42)\n",
    "labels = np.random.choice(['positive', 'negative'], size=len(df), p=[0.5, 0.5])\n",
    "df['label'] = labels\n",
    "\n",
    "# Use the clean text column (adjust column name)\n",
    "df = pd.DataFrame({\n",
    "    'text': df_textos['clear_text'],  # Your cleaned text column\n",
    "    'label': df['label']\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "print(df['label'].value_counts())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81357256",
   "metadata": {},
   "source": [
    "Split Data into Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f842c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5157, Test size: 1720\n",
      "Train labels: label\n",
      "positive    2591\n",
      "negative    2566\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df['text'], \n",
    "    df['label'],\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=df['label']  # Keep class balance\n",
    ")\n",
    "x_test = x_test.fillna('') \n",
    "print(f\"Train size: {len(x_train)}, Test size: {len(x_test)}\")\n",
    "print(\"Train labels:\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f128df",
   "metadata": {},
   "source": [
    "Baseline - CountVectorizer with Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83bbf913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: CountVectorizer (Unigrams) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.47      0.48       856\n",
      "    positive       0.49      0.50      0.49       864\n",
      "\n",
      "    accuracy                           0.49      1720\n",
      "   macro avg       0.49      0.49      0.49      1720\n",
      "weighted avg       0.49      0.49      0.49      1720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Transform training data\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Transform test data and predict\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "predictions = model.predict(x_test_vectorized)\n",
    "\n",
    "# Results\n",
    "print(\"=== Baseline: CountVectorizer (Unigrams) ===\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dad120",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization (Unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "906fef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF (Unigrams) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.47      0.48       856\n",
      "    positive       0.49      0.51      0.50       864\n",
      "\n",
      "    accuracy                           0.49      1720\n",
      "   macro avg       0.49      0.49      0.49      1720\n",
      "weighted avg       0.49      0.49      0.49      1720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Transform and train\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "model_tfidf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_tfidf.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "predictions_tfidf = model_tfidf.predict(x_test_tfidf)\n",
    "\n",
    "print(\"=== TF-IDF (Unigrams) ===\")\n",
    "print(classification_report(y_test, predictions_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c84ac2",
   "metadata": {},
   "source": [
    ": TF-IDF with Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a06dd51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF (2, 2) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.45      0.48       856\n",
      "    positive       0.51      0.58      0.54       864\n",
      "\n",
      "    accuracy                           0.51      1720\n",
      "   macro avg       0.51      0.51      0.51      1720\n",
      "weighted avg       0.51      0.51      0.51      1720\n",
      "\n",
      "=== TF-IDF (3, 3) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.40      0.45       856\n",
      "    positive       0.51      0.62      0.56       864\n",
      "\n",
      "    accuracy                           0.51      1720\n",
      "   macro avg       0.51      0.51      0.50      1720\n",
      "weighted avg       0.51      0.51      0.50      1720\n",
      "\n",
      "=== TF-IDF (5, 5) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.02      0.04       856\n",
      "    positive       0.50      0.97      0.66       864\n",
      "\n",
      "    accuracy                           0.50      1720\n",
      "   macro avg       0.48      0.50      0.35      1720\n",
      "weighted avg       0.48      0.50      0.35      1720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test different n-gram ranges\n",
    "for ngram_range in [(2, 2), (3, 3), (5,5)]:  # Bigrams, then Trigrams\n",
    "    # Create vectorizer\n",
    "    tfidf_ngram = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    \n",
    "    # Train\n",
    "    x_train_ngram = tfidf_ngram.fit_transform(x_train)\n",
    "    model_ngram = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model_ngram.fit(x_train_ngram, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    x_test_ngram = tfidf_ngram.transform(x_test)\n",
    "    predictions_ngram = model_ngram.predict(x_test_ngram)\n",
    "    \n",
    "    print(f\"=== TF-IDF {ngram_range} ===\")\n",
    "    print(classification_report(y_test, predictions_ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca8eaa",
   "metadata": {},
   "source": [
    ": XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b89a0de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost Classifier ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.46      0.48       856\n",
      "    positive       0.50      0.53      0.51       864\n",
      "\n",
      "    accuracy                           0.50      1720\n",
      "   macro avg       0.50      0.50      0.49      1720\n",
      "weighted avg       0.50      0.50      0.49      1720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First install if needed: !pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert labels to numbers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_num = label_encoder.fit_transform(y_train)  # negative=0, positive=1\n",
    "y_test_num = label_encoder.transform(y_test)\n",
    "\n",
    "# Train XGBoost on TF-IDF features (unigrams)\n",
    "xgb_model = XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.2,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=1.0,\n",
    "    objective='binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27\n",
    ")\n",
    "\n",
    "# Use TF-IDF unigrams from Step 4\n",
    "xgb_model.fit(x_train_tfidf, y_train_num)\n",
    "\n",
    "# Predict and convert back to text labels\n",
    "xgb_predictions_num = xgb_model.predict(x_test_tfidf)\n",
    "xgb_predictions = label_encoder.inverse_transform(xgb_predictions_num)\n",
    "\n",
    "print(\"=== XGBoost Classifier ===\")\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15e52f",
   "metadata": {},
   "source": [
    "Alternative Classifier - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e40347e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Classifier ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.36      0.42       856\n",
      "    positive       0.50      0.63      0.56       864\n",
      "\n",
      "    accuracy                           0.50      1720\n",
      "   macro avg       0.50      0.50      0.49      1720\n",
      "weighted avg       0.50      0.50      0.49      1720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(x_train_tfidf, y_train)\n",
    "rf_predictions = rf_model.predict(x_test_tfidf)\n",
    "\n",
    "print(\"=== Random Forest Classifier ===\")\n",
    "print(classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43bd3d",
   "metadata": {},
   "source": [
    "# Compare All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "727d7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPARISON OF ALL MODELS\n",
      "==================================================\n",
      "Model                     Accuracy   F1-Score  \n",
      "---------------------------------------------\n",
      "Baseline (Unigrams)       0.4951     0.4949\n",
      "TF-IDF (Unigrams)         0.4939     0.4934\n",
      "TF-IDF (Trigrams)         0.5177     0.5130\n",
      "XGBoost                   0.5020     0.5014\n",
      "Random Forest             0.4846     0.4777\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON OF ALL MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Collect all predictions\n",
    "models = {\n",
    "    \"Baseline (Unigrams)\": predictions,\n",
    "    \"TF-IDF (Unigrams)\": predictions_tfidf,\n",
    "    #\"TF-IDF (Bigrams)\": predictions_ngram_2,\n",
    "    \"TF-IDF (Trigrams)\": predictions_ngram,\n",
    "    \"XGBoost\": xgb_predictions,\n",
    "    \"Random Forest\": rf_predictions\n",
    "}\n",
    "\n",
    "# Simple comparison table\n",
    "print(f\"{'Model':<25} {'Accuracy':<10} {'F1-Score':<10}\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "for name, pred in models.items():\n",
    "    # Calculate accuracy\n",
    "    accuracy = (pred == y_test).mean()\n",
    "    \n",
    "    # Calculate F1-score (average of both classes)\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(y_test, pred, average='macro')\n",
    "    \n",
    "    print(f\"{name:<25} {accuracy:.4f}     {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
